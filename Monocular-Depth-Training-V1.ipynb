{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":2002504,"sourceType":"datasetVersion","datasetId":1198025}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport torch\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as mt\nfrom PIL import Image\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom transformers import AutoImageProcessor, AutoModel\nimport cv2\nimport warnings\nfrom torch.utils.data import Dataset,DataLoader,random_split\nfrom IPython.display import clear_output\nimport matplotlib.pyplot as plt \nwarnings.filterwarnings(\"ignore\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install mediapy","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data_frame=pd.read_csv('/kaggle/input/nyu-depth-v2/nyu_data/data/nyu2_train.csv')[:2000]\ntrain_size=int(0.6*len(data_frame))\nval_size=int(0.2*len(data_frame))\ntest_size=int(len(data_frame)-train_size-val_size)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train,val,test=random_split(data_frame,[train_size,test_size,val_size])\ntrain_data=data_frame.iloc[train.indices]\nval_data=data_frame.iloc[val.indices]\ntest_data=data_frame.iloc[test.indices]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torchmetrics.image import StructuralSimilarityIndexMeasure as SSIM\nfrom torchmetrics.regression import MeanSquaredError as MSE","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"processor = AutoImageProcessor.from_pretrained('facebook/dinov2-small')\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nclass depthData(Dataset):\n    def __init__(self,img_pd,preprocess=None):\n        self.img_pd=img_pd\n        \n        self.preprocess=preprocess\n\n    def __getitem__(self, idx):\n        # Access a row in the DataFrame\n        sample_i = self.img_pd.iloc[idx]\n        \n        # Get file names\n        img_path = os.path.join('/kaggle/input/nyu-depth-v2/nyu_data', sample_i[0])\n        depth_path = os.path.join('/kaggle/input/nyu-depth-v2/nyu_data', sample_i[1])\n\n        # Load the images\n        # img = torch.tensor(np.array(Image.open(img_path).convert(\"RGB\").resize((504,378))),dtype=float ) # Convert to RGB\n        img =Image.open(img_path) # Convert to RGB\n        # depth = torch.tensor(np.array(Image.open(depth_path).resize((244,244))),dtype=float ) # 16-bit depth image\n        depth = torch.tensor(np.array(Image.open(depth_path).resize((256,256))),dtype=float ) # 16-bit depth image\n\n        # Optional preprocessing\n        if self.preprocess:\n            img = self.preprocess(img)['pixel_values'][0]\n            depth=(depth-depth.min())/(depth.max()-depth.min())\n\n        return img, depth\n    def __len__(self):\n        return len(self.img_pd)\n\n\ntrain_dataset=depthData(train_data,processor)\nval_dataset=depthData(val_data,processor)\ntest_dataset=depthData(test_data,processor)\n\n    ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"batch_size=8\ntrain_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\nbatch_size=8\nval_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\ntest_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"len(train_dataloader)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class EncoderWrapper(nn.Module):\n    def __init__(self,encoder=None):\n        super().__init__()\n        self.encoder=encoder\n\n    def forward(self,x,output_hidden_states=True):\n        return self.encoder(pixel_values=x,output_hidden_states=output_hidden_states)\n\n    def get_intermediate_layers(self,x,layers):\n        hidden_states=self.forward(x).hidden_states\n        return [(hidden_states[i][:,1:,:],hidden_states[i][:,0,:]) for i in layers]\n       \n        ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def checkzero(tensor,msg='zeros'):\n    if torch.all(torch.abs(tensor) < 1e-6):\n        print(msg)\n    ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nclass Att(nn.Module):\n    def __init__(self):\n        super(Att, self).__init__()\n        self.fc1 = nn.Linear(384, 100)\n        self.fc2 = nn.Linear(384,100)\n        \n        \n\n        self.bn = nn.BatchNorm2d(1)\n        self.relu = nn.LeakyReLU(inplace=True)\n\n       \n       \n        self.upconv1=nn.Conv2d(1,128,kernel_size=3,padding=1)\n        self.upconv2=nn.Conv2d(128,1,kernel_size=3,padding=1)\n       \n    def forward(self, x):\n        # x: (B, 256, 384)\n        f1 = self.fc1(x)\n        f2 = self.fc2(x)\n        f1 = torch.bmm(f1, f2.transpose(1, 2))  # (B, 256, 256)\n        f1 = f1.unsqueeze(1)  # (B, 1, 256, 256)\n        \n        \n        f1 = self.bn(f1)\n        f1 = self.relu(f1)\n        f1 = self.upconv1(f1)\n        f1 = self.upconv2(f1)\n        f1 = self.relu(f1)\n        \n       \n        return f1  \n\n\n\n\n\nclass ResFusion(nn.Module):\n    def __init__(self):\n        super(ResFusion, self).__init__()\n        self.conv1 = nn.Conv2d(1, 128, kernel_size=3,padding=1)\n        self.conv2 = nn.Conv2d(128, 1, kernel_size=3, padding=1)\n        \n        \n        self.bn = nn.BatchNorm2d(1)\n        self.relu = nn.LeakyReLU(inplace=True)\n\n    def forward(self, x):\n        out = self.conv1(x)\n        out = self.relu(out)\n        out = self.conv2(out)\n       \n        out = self.bn(out)\n        out = self.relu(out)\n        return x + out\n\n\nclass Decoder(nn.Module):\n    def __init__(self):\n        super(Decoder, self).__init__()\n        self.attention = nn.ModuleList([Att() for _ in range(4)])\n\n        self.resF1 = ResFusion()\n        self.resF2 = ResFusion()\n        self.resF3 = ResFusion()\n        self.resF4 = ResFusion()\n\n    def forward(self, features):\n       \n        # Compute and fuse attention outputs in-place\n        d4 = self.resF4(self.attention[3](features[3][0]))\n        d3 = d4 + self.resF3(self.attention[2](features[2][0]))\n        d2 = d3 + self.resF2(self.attention[1](features[1][0]))\n        d1 = d2 + self.resF1(self.attention[0](features[0][0]))\n\n\n        return d1  # final output: (B, 1, 256, 256)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"encoder = AutoModel.from_pretrained('facebook/dinov2-small')\nencoder=EncoderWrapper(encoder=encoder)\ndecoder=Decoder()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def init_weights(m):\n    if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n        nn.init.kaiming_normal_(m.weight)\n        if m.bias is not None:\n            nn.init.constant_(m.bias, 0)\n\ndecoder.apply(init_weights)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"device=torch.device('cuda:0')\nlmse=MSE().to(device)\ndecoder=decoder.to(device)\nencoder=encoder.to(device)\ndecoder.train()\noptimizer=torch.optim.Adam(decoder.parameters(),lr=0.005)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"epochs=6\ntrain_loss=[]\nval_loss=[]\ntrain_epoch_loss=0\nval_epoch_loss=0\nfor e in range(epochs):\n    print(f'----------------------epoch: {e}-----------------')\n    for i,(img_batch,depth_batch) in enumerate(train_dataloader):\n        optimizer.zero_grad()\n        img_batch=img_batch.to(device,non_blocking=True)\n        depth_batch=depth_batch.to(device,non_blocking=True)\n        # img_batch=img_batch.to(device)\n        # depth_batch=depth_batch.to(device)\n        with torch.no_grad():      \n            patch_h, patch_w = img_batch.shape[-2] // 14, img_batch.shape[-1] // 14\n            features = encoder.get_intermediate_layers(img_batch,[2,5,8, 11])\n            del img_batch\n    \n        y_pred=decoder(features)\n        y_pred=torch.tanh(y_pred)+1\n        loss_mae=lmse(y_pred,depth_batch.unsqueeze(1))\n        loss=loss_mae\n        \n        \n        \n        loss.backward()\n        optimizer.step()\n        if (i%16==0):\n            # print(f'Batch {i},Loss:{loss.item()},lsim:{loss_sim.item()},lsilog:{loss_silog.item()}')\n             print(f'Batch {i}*************************************************')\n             print(f'Loss:{loss.item()}')\n             print(f'mae:{loss_mae.item()}')\n             \n             # for name, param in decoder.named_parameters():\n             #    if param.grad is not None:\n             #        print(f\"{name} grad mean: {param.grad.mean().item()}, max: {param.grad.max().item()}\")\n\n        del loss,y_pred,depth_batch,features\n        train_epoch_loss+=loss_mae.cpu().detach().item()\n        del loss_mae\n        \n        \n        torch.cuda.empty_cache()\n    print(i)    \n    train_loss.append(train_epoch_loss/(i+1))\n   \n\n    \n    i=0\n    for i,(img_batch,depth_batch) in enumerate(val_dataloader):\n       \n        img_batch=img_batch.to(device,non_blocking=True)\n        depth_batch=depth_batch.to(device,non_blocking=True)\n       \n        with torch.no_grad():      \n            patch_h, patch_w = img_batch.shape[-2] // 14, img_batch.shape[-1] // 14\n            features = encoder.get_intermediate_layers(img_batch,[2,5,8, 11])\n            del img_batch\n    \n            y_pred=decoder(features)\n            y_pred=torch.tanh(y_pred)+1\n            loss_mae=lmse(y_pred,depth_batch.unsqueeze(1))\n            loss=loss_mae\n        \n        \n        \n       \n        del loss,y_pred,depth_batch,features,\n        val_epoch_loss+=loss_mae.cpu().detach().item()\n        del loss_mae\n\n        \n        torch.cuda.empty_cache()\n    print(i)\n    val_loss.append(val_epoch_loss/(i+1))\n    train_epoch_loss=0\n    val_epoch_loss=0\n\n    clear_output(wait=True)\n    plt.figure(figsize=(5,3))\n    plt.plot(train_loss,label='Train loss',marker='o')\n    plt.plot(val_loss,label='Validation loss',marker='o')\n    plt.title('Training and Validation Loss')\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.legend()\n    plt.grid(True) \n    plt.tight_layout()\n    plt.show()\n            \n        \n        \n        \n        ","metadata":{"trusted":true,"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_epoch_loss=0\nfor i,(img_batch,depth_batch) in enumerate(test_dataloader):\n       \n        img_batch=img_batch.to(device,non_blocking=True)\n        depth_batch=depth_batch.to(device,non_blocking=True)\n       \n        with torch.no_grad():      \n            patch_h, patch_w = img_batch.shape[-2] // 14, img_batch.shape[-1] // 14\n            features = encoder.get_intermediate_layers(img_batch,[2,5,8, 11])\n            del img_batch\n    \n            y_pred=decoder(features)\n            y_pred=torch.tanh(y_pred)+1\n            loss_mae=lmse(y_pred,depth_batch.unsqueeze(1))\n            loss=loss_mae\n        \n        \n        \n       \n        del loss,y_pred,depth_batch,features,\n        test_epoch_loss+=loss_mae.cpu().detach().item()\n        del loss_mae\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_epoch_loss","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"val_loss[5]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def get_sample_ind(ind):\n    \n    sample_i = data_frame.iloc[ind]\n        \n    # Get file names\n    img_path = os.path.join('/kaggle/input/nyu-depth-v2/nyu_data', sample_i[0])\n    depth_path = os.path.join('/kaggle/input/nyu-depth-v2/nyu_data', sample_i[1])\n    \n    # Load the images\n   \n    img = Image.open(img_path).convert(\"RGB\") # Convert to RGB\n    img=processor(img,return_tensors=\"pt\")['pixel_values'][0]\n    depth = torch.tensor(np.array(Image.open(depth_path).resize((224,224))),dtype=float ) # 16-bit depth image\n    depth=(depth-depth.min())/(depth.max()-depth.min())\n    return img,depth\n\n    \n    ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if torch.cuda.is_available():\n    device=torch.device('cuda:0')\nelse:\n    device=torch.device('cpu')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import mediapy as mp\nimport torch\n\ndef show(img):\n\n    \n    tensor_img=img.cpu().numpy()\n\n    # Normalize to [0,255] uint8\n    img_norm = (tensor_img - tensor_img.min()) / (tensor_img.max() - tensor_img.min() + 1e-8)\n    img_uint8 = (img_norm * 255).astype('uint8')\n    \n    # Show with mediapy\n    mp.show_image(img_uint8, title=\"Depth Prediction\")\n\ndef get_pred(x):\n   \n    with torch.no_grad():\n       \n        \n        patch_h, patch_w = x.shape[-2] // 14, x.shape[-1] // 14\n        features = encoder.get_intermediate_layers(x,[2, 5, 8, 11])\n        y_pred=decoder(features)\n    return y_pred[0]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"testimg,testdepth=get_sample_ind(189)\npred=get_pred(testimg.to(device).unsqueeze(0))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"torch.tanh(pred.squeeze(0))+1","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"testdepth","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"show(torch.tanh(pred.squeeze(0))+1)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"show(testdepth)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}